{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078a299",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import yaml, os\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "\n",
    "if \"dev\" in notebook_path:\n",
    "    origen_env = \"dev\"\n",
    "    destino_env = \"pre\"\n",
    "elif \"pre\" in notebook_path:\n",
    "    origen_env = \"pre\"\n",
    "    destino_env = \"prod\"\n",
    "else:\n",
    "    print(\"‚ùå Entorno no soportado\")\n",
    "    dbutils.notebook.exit(\"\")\n",
    "\n",
    "# Ajustamos la ruta base al nivel superior donde est√°n notebooks, tables.txt y schemas/\n",
    "base_path = os.path.dirname(os.path.dirname(notebook_path))  # Sube dos niveles desde notebooks/\n",
    "\n",
    "tables_path = os.path.join(base_path, \"tables.txt\")\n",
    "schemas_base = os.path.join(base_path, \"schemas\")\n",
    "dbutils.widgets.text(\"CATALOGO\", \"valor_por_defecto\")\n",
    "catalog = dbutils.widgets.get(\"CATALOGO\")\n",
    "print(f\"CATALOGO recibido: {catalog}\")\n",
    "\n",
    "tablas_raw = [line.strip() for line in open(tables_path) if line.strip()]\n",
    "tablas = [t.replace(\"__CATALOGO__\", catalog) for t in tablas_raw]\n",
    "\n",
    "for tabla_full in tablas:\n",
    "    print(f\"üîÅ Migrando {tabla_full} ({origen_env} ‚Üí {destino_env})\")\n",
    "\n",
    "    schema_path = os.path.join(schemas_base, f\"{tabla_full}.yaml\".replace(\".\", \"_\"))\n",
    "\n",
    "    if not os.path.exists(schema_path):\n",
    "        print(f\"‚ö†Ô∏è Falta esquema para {tabla_full}\")\n",
    "        continue\n",
    "\n",
    "    with open(schema_path, \"r\") as f:\n",
    "        schema_config = yaml.safe_load(f)\n",
    "\n",
    "    fields = []\n",
    "    for col in schema_config[\"columns\"]:\n",
    "        col_type = col[\"type\"].lower()\n",
    "        dtype = {\n",
    "            \"string\": StringType(),\n",
    "            \"boolean\": BooleanType(),\n",
    "            \"double\": DoubleType(),\n",
    "            \"integer\": IntegerType(),\n",
    "            \"long\": LongType()\n",
    "        }.get(col_type)\n",
    "\n",
    "        if dtype is None:\n",
    "            raise Exception(f\"‚ùå Tipo no soportado: {col_type}\")\n",
    "        fields.append(StructField(col[\"name\"], dtype, col[\"nullable\"]))\n",
    "\n",
    "    schema = StructType(fields)\n",
    "\n",
    "    try:\n",
    "        spark.read.table(tabla_full)\n",
    "        print(\"‚úÖ Tabla ya existe\")\n",
    "    except:\n",
    "        print(\"üìê Creando tabla\")\n",
    "        spark.createDataFrame([], schema).write.format(\"delta\").saveAsTable(tabla_full)\n",
    "\n",
    "    try:\n",
    "        df_origen = spark.read.table(tabla_full).filter(f\"env = '{origen_env}'\")\n",
    "        if df_origen.rdd.isEmpty():\n",
    "            print(\"‚ÑπÔ∏è Nada para migrar\")\n",
    "            continue\n",
    "\n",
    "        df_destino = df_origen.drop(\"env\").withColumn(\"env\", lit(destino_env))\n",
    "\n",
    "        delta_table = DeltaTable.forName(spark, tabla_full)\n",
    "        delta_table.alias(\"t\") \\\n",
    "            .merge(df_destino.alias(\"s\"), \"t.id = s.id AND t.env = s.env\") \\\n",
    "            .whenMatchedUpdateAll() \\\n",
    "            .whenNotMatchedInsertAll() \\\n",
    "            .execute()\n",
    "        print(\"‚úÖ Migraci√≥n exitosa\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
